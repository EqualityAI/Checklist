<img src="https://github.com/EqualityAI/Checklist/blob/master/img/collogo.png" align="right" alt="EqualityAI Logo" width="120" />

# Equality AI Responsible MLOps Toolkit 
In 2019, a [major research study]() revealed that millions of Black patients with complex medical needs did not qualify for extra care, despite being considerably sicker than White patients. This groundbreaking study confirmed the unfortunate truth that healthcare decision-making is not always fair and access to healthcare is often unequal. Medical professionals and researchers are beginning to use machine learning algorithms (ML), a subset of artificial intelligence, to assist in their medical decisions. However, ML models built on real-world data might unintentionally incorporate inherent unfairness and strengthen the disparity in medical decisions. 

As the developers that make the models, we must accept more responsibility and ensure these products are being applied fairly.  The way to do that is a human-centered approach.  Responsible AI is an emerging framework that addresses this need and helps mitigate the potential risks of harm from AI and includes ethics, fairness, accuracy, security, and privacy.  

Equality AI is the first organization to begin assembling the Responsible AI framework into an end-to-end Responsible MLOPs Studio. The technology behind our Responsible MLOPs Studio is an open source ML software framework and tool, called `responsible_mlops’, with additional functions  that can be selectively incorporated to create various workflows for fitting responsible models.<br />

## `responsible_mlops` 
To make steps in our `responsible_mlops` easy to follow, we have likened these various workflows to something everyone can understand—a recipe. These recipes outline the “ingredients” you need and the exact steps to take to ensure you’ve cooked up a fair AI system. [Our first recipe is a fair preprocessing ML recipe]() and the main goal of this recipe is to repair the data set on which the model is run (pre-processing).<br />

Sign up to hear when we release recipes that will tackle adjustments to the model (in processing) and the predictions (post-processing).

## Highlighted functions
`fairness_tree_metric` 
`mitigation_method_mapping`

## Responsible AI Takes a Community
We are starting with fairness, and it doesn’t end there. We have much more in the works,  and we want to know—what do you need? Do you have a Responsible AI challenge you need to solve? Drop us a line and let’s see how we can help!

## Contributing to the project
